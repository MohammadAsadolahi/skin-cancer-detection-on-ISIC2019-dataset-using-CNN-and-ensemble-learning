{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1193409,"sourceType":"datasetVersion","datasetId":679322}],"dockerImageVersionId":30235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\n!pip3 install --upgrade imutils\nimport cv2\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-11-24T05:51:39.093222Z","iopub.execute_input":"2024-11-24T05:51:39.093537Z","iopub.status.idle":"2024-11-24T05:51:49.879325Z","shell.execute_reply.started":"2024-11-24T05:51:39.093464Z","shell.execute_reply":"2024-11-24T05:51:49.878299Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_info = pd.read_csv(\"../input/isic-2019/ISIC_2019_Training_GroundTruth.csv\")\ndel dataset_info[dataset_info.columns[-1]]\nadress_list=[]\nfor i in range(1,9):\n    adress_list.append(dataset_info[dataset_info[dataset_info.columns[i]]==1].image)\nfor i in range(1,9):\n    os.makedirs(f\"Dataset/{i-1}/{i-1}\")\nfor i in range(1,9):\n    os.makedirs(f\"DatasetAugmented/{i-1}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-24T05:51:49.881174Z","iopub.execute_input":"2024-11-24T05:51:49.881475Z","iopub.status.idle":"2024-11-24T05:51:49.969015Z","shell.execute_reply.started":"2024-11-24T05:51:49.881446Z","shell.execute_reply":"2024-11-24T05:51:49.968106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_size=256\nfor i in range(0,8):\n#     counter=0\n    for j in range(0,len(adress_list[i])):\n        src_path = '../input/isic-2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input/'+ adress_list[i].iloc[j] + '.jpg'\n        dst_path = f\"Dataset/{i}/{i}/\"+adress_list[i].iloc[j] + '.png'\n        image = cv2.resize(cv2.imread(src_path),dsize=(input_size,input_size))\n        cv2.imwrite(dst_path,image)\n        del image\n#         counter+=1\n#         if counter>9:\n#             break\n    print(f\"category{i} done\")","metadata":{"execution":{"iopub.status.busy":"2024-11-24T05:51:49.970265Z","iopub.execute_input":"2024-11-24T05:51:49.970972Z","iopub.status.idle":"2024-11-24T06:02:47.013725Z","shell.execute_reply.started":"2024-11-24T05:51:49.970934Z","shell.execute_reply":"2024-11-24T06:02:47.012665Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"category=['MEL','NV','BCC','AK','BKL','DF','VASC','SCC']\nrandom_items=[random.randint(0,max) for max in [len(i) for i in adress_list]]\nsrc_path=[]\nfor i,j in  zip(range(9),random_items):\n    src_path.append('../input/isic-2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input/'+ adress_list[i].iloc[j] + '.jpg')\nplt.figure(figsize=(80,60))\nfor i,t in zip(range(8),category):\n    plt.subplot(2,4,i+1)\n    plt.title(t,fontsize=120,fontweight='bold')\n    plt.imshow(np.asarray(cv2.resize(plt.imread(src_path[i]),dsize=(400,400))))\n    plt.axis('off')\nplt.savefig('dataset_diffrent_categories.png', dpi=199)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T06:02:47.016502Z","iopub.execute_input":"2024-11-24T06:02:47.017232Z","iopub.status.idle":"2024-11-24T06:02:47.021307Z","shell.execute_reply.started":"2024-11-24T06:02:47.017185Z","shell.execute_reply":"2024-11-24T06:02:47.020396Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.preprocessing import image\nfor i in range(8):\n    original_path=f'Dataset/{i}/'\n    agumented_path=f'DatasetAugmented/{i}/'\n    step=10#23\n    bach_size=int(len(adress_list[i])/step)\n    data_generator=image.ImageDataGenerator(brightness_range=[0.2,0.4]).flow_from_directory(original_path,batch_size=bach_size,save_to_dir=agumented_path,save_prefix='b',target_size=(input_size,input_size))\n    for i in range(step):\n        data_generator.next()\n    del data_generator\n    data_generator=image.ImageDataGenerator(rotation_range=0.2).flow_from_directory(original_path,batch_size=bach_size,save_to_dir=agumented_path,save_prefix='r',target_size=(input_size,input_size))\n    for i in range(step):\n        data_generator.next()\n    del data_generator\n    data_generator=image.ImageDataGenerator(width_shift_range=0.2).flow_from_directory(original_path,batch_size=bach_size,save_to_dir=agumented_path,save_prefix='w',target_size=(input_size,input_size))\n    for i in range(step):\n        data_generator.next()\n    del data_generator\n    data_generator=image.ImageDataGenerator(height_shift_range=0.2).flow_from_directory(original_path,batch_size=bach_size,save_to_dir=agumented_path,save_prefix='h',target_size=(input_size,input_size))\n    for i in range(step):\n        data_generator.next()\n    del data_generator\nimport glob\nimport shutil\nfor i in range(8):\n    augmented_image_list=glob.glob(f'DatasetAugmented/{i}/*.png')\n    for image in augmented_image_list:\n        shutil.move(image,f'Dataset/{i}/{i}/')\nshutil.rmtree('DatasetAugmented')\nprint(\"moved successfuly\")\n# shutil.make_archive(\"augmentedDataset\", 'zip', 'Dataset/')\n# shutil.make_archive(\"Dataset\", 'zip', 'Dataset/') ","metadata":{"execution":{"iopub.status.busy":"2024-11-24T06:02:47.022333Z","iopub.execute_input":"2024-11-24T06:02:47.022631Z","iopub.status.idle":"2024-11-24T06:17:47.072066Z","shell.execute_reply.started":"2024-11-24T06:02:47.022586Z","shell.execute_reply":"2024-11-24T06:17:47.071062Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport shutil\nall_images_address=[]\nall_image_count=[]\nfor i in range(8):\n    augmented_image_list=glob.glob(f'Dataset/{i}/{i}/*.png')\n    all_image_count.append(len(augmented_image_list))\n    all_images_address+=augmented_image_list\nlabels=[]\ncat=0\nfor category in all_image_count:\n    for i in range(category):\n        labels.append(cat)\n    cat+=1\ntrain_address= np.array(all_images_address, dtype=object)\nlabels=np.expand_dims(np.array(labels),axis=1)\ntrain_data=np.array([np.array(cv2.imread(image))/np.max(np.array(cv2.imread(image))) for image in train_address])\nprint(labels.shape)\nprint(train_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T06:17:47.073609Z","iopub.execute_input":"2024-11-24T06:17:47.074342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor as outlier_detector\nprint(train_data.shape)\nprint(labels.shape)\nmodel=outlier_detector()\npredicts=model.fit_predict(train_data.reshape(train_data.shape[0],-1),labels)\noutlier_indicies=np.where(predicts==-1)[0]\ntrain_data=np.delete(train_data, outlier_indicies,axis=0)\nlabels=np.delete(labels, outlier_indicies)\nprint(train_data.shape)\nprint(labels.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve,classification_report,confusion_matrix,auc,accuracy_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom keras.models import load_model\nfrom keras.utils import np_utils\nfrom numpy import dstack\nimport os\nimport glob\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import classification_report\nimport datetime\ndef XGB(x_train,y_train,x_test,y_test,members):\n    \n    def stacked_dataset(members, x_train):\n        stackX = None\n        for model in members:\n            yhat = model.predict(x_train, verbose=0)\n            if stackX is None:\n                stackX = yhat\n            else:\n                stackX = dstack((stackX, yhat))\n        stackX = stackX.reshape((stackX.shape[0], stackX.shape[1] * stackX.shape[2]))\n        return stackX\n\n    def fit_stacked_model(members,x_train,y_train):\n        stackedX=stacked_dataset(members,x_train)\n        model=XGBClassifier()\n        model.fit(stackedX,y_train)\n        return model\n\n\n    def stacked_prediction(members, model, inputX):\n        stackedX = stacked_dataset(members, inputX)\n        yhat = model.predict(stackedX)\n        return yhat\n    \n    print(\"entering XGB\")\n    all_models = list()\n    for i in range(2):\n        filename = f'./CNN_fold' + str(i + 1) + '.h5'\n        model = load_model(filename)\n        all_models.append(model)  \n    \n    \n    model = fit_stacked_model(all_models, x_train, y_train)\n    yhat = stacked_prediction(all_models, model, x_test)\n    acc1 = accuracy_score(y_test, yhat)\n    print(f\"accuracy is : {acc1}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D,Dropout\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\nimport tensorflow\nimport copy\n\ndef buildCNN(input_size):\n    model=Sequential()\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=2, input_shape=(input_size,input_size,3)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(512,3,padding='same',activation='relu',strides=2))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=2))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=1))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(8, activation='softmax'))\n    \n    model.compile(loss=tensorflow.keras.losses.sparse_categorical_crossentropy,\n                optimizer=\"Adam\",\n                metrics=['accuracy'])\n    return model\n\n# def train_model(train_batch_adresses,labels,ep,batch_size,model1,model2):\n#     labels= copy.deepcopy(labels)\n#     np.random.shuffle(labels)\n#     for j in range(ep):\n#         for i in range(int(train_batch_adresses.shape[0]/batch_size)-1):\n#                 X= np.array([np.array(cv2.imread(image)) for image in train_batch_adresses[i*batch_size:(i+1)*batch_size]])\n#                 Y= labels[i*batch_size:(i+1)*batch_size]\n#                 model1.fit(X,Y,epochs=1,verbose=0)\n#                 model2.fit(X,Y,epochs=1,verbose=0)\n#                 del X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ndef XGB(x_train,y_train,x_test,y_test):\n    \n    def load_all_models(n_models):\n        all_models = list()\n        for i in range(n_models):\n            filename = f'./CNN_fold' + str(i + 1) + '.h5'\n            model = load_model(filename)\n            all_models.append(model)\n            print('>loaded %s' % filename)\n        return all_models\n    \n    def stacked_dataset(members, inputX):\n        stackX = None\n        for model in members:\n            yhat = model.predict(inputX, verbose=0)\n            if stackX is None:\n                stackX = yhat\n            else:\n                stackX = dstack((stackX, yhat))\n        stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n        return stackX\n\n    def fit_stacked_model(members,inputX,inputy):\n        stackedX=stacked_dataset(members,inputX)\n        model=XGBClassifier()\n        start=datetime.datetime.now()\n        model.fit(stackedX,inputy)\n        end=datetime.datetime.now()\n        training_time=end-start\n        return model,training_time\n\n    def stacked_prediction(members, model, inputX):\n        stackedX = stacked_dataset(members, inputX)\n        yhat = model.predict(stackedX)\n        return yhat\n\n    trainX=x_train\n    testX=x_test\n    trainy=y_train\n    testy=y_test\n    \n    n_members = 2\n    members = load_all_models(n_members)\n    print('Loaded %d models' % len(members))\n    # evaluate standalone models on test dataset\n#     for model in members:\n#         testy_enc = testy #testy_enc = np_utils.to_categorical(testy)\n#         loss, acc = model.evaluate(testX, testy_enc, verbose=0)\n    model,training_time = fit_stacked_model(members, trainX, trainy)\n    yhat = stacked_prediction(members, model, testX)\n    predicts=yhat\n    actuals=testy\n    acc1 = accuracy_score(actuals, predicts)\n    print(f\"xgb acc:{acc1}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folds_count=10\nkfold = KFold(n_splits=folds_count, shuffle=True,random_state=None)\nfor train, test in kfold.split(train_data, labels):\n    models=[]\n    inner_kfold = KFold(2,shuffle=True,random_state=None)\n    x_data=train_data[train]\n    y_data=labels[train]\n    counter=1\n    for t, v in inner_kfold.split(x_data,y_data):\n        inner_x_train=x_data[t]\n        inner_y_train=y_data[t]\n        inner_x_test=x_data[v]\n        inner_y_test=y_data[v]\n        model=buildCNN(input_size)\n        model.fit(inner_x_train,inner_y_train,epochs=60,batch_size=256,validation_data=(inner_x_test,inner_y_test),verbose=0)#,verbose=0\n        print(f\"inner result:{model.evaluate(inner_x_test,inner_y_test, verbose=0)}\")\n        model.save(f'./CNN_fold{counter}.h5')\n        models.append(model)\n        counter+=1\n    print(f\"outer cNN results:{models[0].evaluate(train_data[test],labels[test], verbose=0),models[0].evaluate(train_data[test],labels[test], verbose=0)}\")\n#     XGB(inner_x_train,inner_y_train,train_data[test],labels[test],models)\n    XGB(train_data[train],labels[train],train_data[test],labels[test])\n\n    \n#     XGB(inner_x_test,inner_y_test,train_data[test],labels[test])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lasti","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    train_model(train_address[train],labels[train],epochs,int((train_address[train].shape[0]/100)),model1,model2)\n    model1_evaluate_results=model1.evaluate(train_data[test],labels[test], verbose=0)\n    print(f\"M1:{model1_evaluate_results}\")\n    XGB(train_data[train],labels[train],train_data[test],labels[test],models)\n    \n    test_x= np.array([np.array(cv2.imread(image)) for image in train_address[test]])\n    model1_evaluate_results=model1.evaluate(test_x, labels[test], verbose=0)\n    model2_evaluate_results=model2.evaluate(test_x, labels[test], verbose=0)\n    print(f\"M1:{after_evaluate_results}      M2:{after_evaluate_results}\")\n    del test_x\nshutil.rmtree('Dataset')\nprint(\"datasetCleaned\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfold_number=1\nkfold=KFold(10,shuffle=True,random_state=None)\nfor train,test in kfold.split(train_data, labels):\n    x_data=x[train]\n    y_data=y[train]\n    x_test=x[test]\n    test_labels=y[test]\n\n    counter=1\n    n_epch=30\n\n    from sklearn.model_selection import KFold\n    kfold = KFold(2,shuffle=True,random_state=None)\n    for train, valid in kfold.split(x_data,y_data):\n        x_train=x_data[train]\n        x_valid=x_data[valid]\n        train_labels=y_data[train]\n        valid_labels=y_data[valid]\n\n        from keras.utils import np_utils\n        y_train=keras.utils.to_categorical(train_labels)\n        y_test=np_utils.to_categorical(test_labels)\n        y_valid=np_utils.to_categorical(valid_labels)\n\n        model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n        net_history=model.fit(x_train, y_train, batch_size=256, epochs=n_epch ,validation_data=(x_valid,y_valid),callbacks=[calback])\n    import XGBoost2\n    XGBoost2.XGB(x_train,x_test,train_labels,test_labels,fold_number)\n\n    fold_number+=1\n\n\nprint('Successfully was done...!')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}